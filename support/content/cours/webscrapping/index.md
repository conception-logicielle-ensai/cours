---
title: "R√©cup√©ration de donn√©es via le webscraping"
weight: 19
draft: false
summary: "Techniques de web scraping et r√©cup√©ration de donn√©es depuis le web."
slug: "webscraping"
tags: ["web scraping", "python", "requests", "beautifulsoup"]
series: ["Cours"]
series_order: 9
---

> [!TIP]+ Acc√®s aux exemples
> Les exemples pr√©sent√©s sont accessibles directement sur le d√©p√¥t git associ√© : https://github.com/conception-logicielle-ensai/exemples-cours/tree/main/architecture-applicative

<img src="https://d1pnnwteuly8z3.cloudfront.net/images/4d5bf260-c3d0-4f21-b718-8ede8d4ca716/febf9de6-8a5a-4055-b274-e685485496f5.jpeg" />

## Introduction au Web Scraping

Le **web scraping** d√©signe les techniques d‚Äô**extraction de contenu depuis des sites internet**. C'est une pratique que l'on envisage lorsque l'on a acc√®s √† des donn√©es publiquement disponibles, mais qu'aucun **fichier fourni ou API** n'est mis √† disposition pour les exploiter.

Elle repose sur l'utilisation de **clients HTTP** pour traiter des donn√©es **non format√©es** pour l'utilisation, typiquement des pages web accessibles aux utilisateurs au format `HTML`.

> Le terme fait g√©n√©ralement r√©f√©rence √† l‚Äôusage de **bots** pour collecter automatiquement ces contenus.

Le web scraping peut s'appr√©hender √† travers des processus de type `ETL` (Extract, Transform, Load) :

* **Extract** : extraction des donn√©es brutes √† partir de pages web.
* **Transform** : traitement des donn√©es brutes pour les rendre exploitables (`parsing`).
* **Load** : stockage des donn√©es dans un fichier, une base de donn√©es ou un autre format exploitable.

### Exemples d'utilisation :

* **Suivi de prix** : march√©s, bourses, sites d'annonces, comparateurs de prix. Exemple : l'INSEE dans la section **IPC**.
* **R√©cup√©ration de donn√©es textuelles massives** : commentaires, tweets, avis Google, etc.
* **R√©cup√©ration de listes de contacts** : annuaires, Pages Jaunes, Pages Blanches, bases de sondage d'entreprises.

## Webscrapping et l√©galit√© : existence d'une zone grise

Le webscraping est l'une des pratiques les plus courantes pour la r√©cup√©ration de donn√©es sur le web. 
Le web scraping permet d‚Äôextraire des donn√©es disponibles publiquement, mais certaines donn√©es sont **prot√©g√©es par le droit d'auteur**, et d'autres concernent des **informations personnelles, soumises √† des r√®gles de confidentialit√©**.
C'est ce qui explique qu'il y a l√† une zone grise.


### Droit des donn√©es :

- En Europe, actuellement il y a la [General Data Protection Regulation (GDPR) (ou RGPD) ](https://www.cnil.fr/fr/reglement-europeen-protection-donnees) qui prot√®gent les donn√©es personnelles mais ne pr√©cisent pas le traitement dans le cas d'usage du webscraping.

- Aux √©tats unis : [California Privacy Rights Act (CPRA)](https://thecpra.org/), c'est une loi en californie, mais rien n'existe c√¥t√© f√©d√©ral sur le droit des donn√©es.

> Il est donc pr√©f√©rable d'utiliser des APIs au maximum pour la r√©cup√©ration de donn√©es lorsque celles-ci sont disponibles.

M√™me si le web scraping est l√©gal sous certaines conditions, les sites web peuvent mettre en place des m√©canismes techniques pour se prot√©ger contre le scraping abusif.

## Protection contre le webscrapping : Humaniser nos processus

La r√©cup√©ration de donn√©es en utilisant du webscrapping peut s'assimiler √† une attaque informatique de type `DDOS` (Denial Of Service).

Ainsi certains sites ont mis en oeuvre des solutions pour se prot√©ger contre le DDOS et le webscraping **abusif**. 
Ces solutions reposent sur les principes suivants : 

- Les sites bloquent des requ√™tes r√©p√©t√©es sur des intervalles de temps trop proches venant d'une m√™me IP (~m√™me machine)
- Les sites modifient contenu des balises html pour emp√™cher l'automatisation
- Cr√©ation de `Honeypot` : liens invisibles que seul un bot "cliquerait" pour bloquer les bots/botteurs.
- Authentification exig√©e au bout d'un certain nombre d'usages.

Ils proposent donc diff√©rentes guidelines g√©n√©rales pour les utilisateurs qui souhaitent webscraper : 

- **Respecter les r√®gles d√©finies par le site** :
   Les sites indiquent souvent ce que les robots sont autoris√©s √† faire via le fichier `robots.txt`. Par exemple : [https://www.google.com/robots.txt](https://www.google.com/robots.txt). Avant de scraper un site, il est important de consulter ce fichier et de respecter ses directives.

- **Espacer les requ√™tes** :
   Pour √©viter de surcharger les serveurs, il est recommand√© de **laisser un intervalle entre chaque requ√™te**. Cela r√©duit les risques de blocage et limite l'impact sur les performances du site.

- **Choisir les p√©riodes de faible trafic** :
   √âviter de lancer des requ√™tes pendant les heures de forte activit√© du site. Par exemple, pour les administrations ou services publics fran√ßais, il est pr√©f√©rable de planifier le scraping **la nuit** ou pendant des p√©riodes o√π le service est moins sollicit√©.


## R√©cup√©ration des donn√©es : Client HTTP

Les clients http sont n√©cessaires pour la r√©cup√©ration des donn√©es expos√©es sur les sites. Il en existe de diff√©rents types.

### Client en ligne de commande

Les outils comme **Wget** et **cURL** permettent d'envoyer des requ√™tes HTTP directement depuis le terminal.

- cURL : Supporte plusieurs protocoles (HTTP, FTP, etc.), permet d'envoyer des requ√™tes GET, POST, etc.
- Wget : Principalement utilis√© pour t√©l√©charger des fichiers depuis le web.

> Ils permettent de scripter des requ√™tes et sont disponibles sur la plupart des OS.


> [!TIP]+ Remarque
> Nous vous avons pr√©sent√© curl dans la session pr√©c√©dente. Il est tr√®s utile puisqu'il est commun d'√©changer des commandes curl entre des √©quipes de d√©veloppeur (DEV) ou des √©quipes de maintenance d'infrastructure (OPS).

### Clients utilitaires

Il existe des clients utilitaires comme **Insomnia** et **Postman** offrent une interface graphique pour tester des API REST. 

- `Postman` : Permet d'envoyer des requ√™tes HTTP, d'automatiser des tests et de documenter des API.
- `Insomnia` : Similaire √† `Postman`, ax√© sur la simplicit√© et l‚Äôexp√©rience utilisateur.


> [!TIP]+ Pour aller plus loin
> Lien vers les sites pour t√©l√©charger/get started avec ces clients http : [Postman](https://www.postman.com/) et  [Insomnia](https://insomnia.rest/)


> Ils peuvent √™tre tr√®s pratiques si vous travaillez avec des coll√®gues ne maitrisant pas python puisque vous pouvez leur partager vos scripts.

### Navigateur et utilisation des outils de d√©veloppement

<img src="https://firefox-source-docs.mozilla.org/_images/pageinspector.png" />

Les navigateurs sont des clients HTTP tr√®s adapt√©s pour effectuer des requ√™tes HTTP. Ils proposent par ailleurs une interface de d√©vtools directement incorpor√©e.

Ils proposent :
- Un onglet R√©seau : Cela permet de traquer les requ√™tes HTTP effectu√©es par le navigateur. Cela peut √™tre utile pour les stocker ou les reproduire via script.
- Un onglet debug : interface debug c√¥t√© client (nos pages executant du javascript, cela permet de comprendre un bug d'affichage par exemple)
- Une console : elle permet d'executer du javascript sur la page
- Un inspecteur : permet de scanner les √©l√©ments et de r√©cup√©rer des informations sur celles ci.

**...Et d'autres fonctionnalit√©s..**

Consultons ensemble cette documentation : 

- [https://firefox-source-docs.mozilla.org/devtools-user/](https://firefox-source-docs.mozilla.org/devtools-user/)


> [!TIP]+ Pour aller plus loin
> Lien vers une vid√©o pr√©sentant les outils de d√©veloppements chrome : [https://www.youtube.com/watch?v=BrsyIyYSP1c](https://www.youtube.com/watch?v=BrsyIyYSP1c)


### Utilisation de `requests`

Pour effectuer des requ√™tes http, on utilisera la librairie client http `requests`.

Les requ√™tes seront cette fois effectu√©es pour la r√©cup√©ration de pages web, pour **r√©cup√©rer des fichiers HTML bruts**, et donc on privil√©giera la r√©cup√©ration du text dans les r√©ponses : 

```py
import requests

URL_COURS = "https://conception-logicielle.abrunetti.fr/"
response = requests.get(url)
print(response.text)  # Affiche le contenu HTML de la page
```

### Utilisation d'un script dans une page HTML (Javascript)

En javascript il existe de nombreux client HTTP, nous privil√©gierons l'usage d'axios en seconde partie du cours, mais le client http "de base" est **fetch** :

Observez plut√¥t en r√©cup√©rant le code source de la **page d'accueil** du cours :

<div id="fetch-with-fetch">
<button id="fetch-btn" style="display: inline-block; padding: 10px 20px; background-color: #007bff; color: white; font-size: 16px; border-radius: 5px; cursor: pointer; text-align: center; font-family: Arial, sans-serif;" onclick="fetchPageFetch()">Charger la page avec le package fetch</button>
<div id="fetch-content" style="white-space: pre-wrap; border: 1px solid #000; padding: 10px; margin-top: 10px;"></div>

<script>
async function fetchPageFetch() {
    const url = "https://conception-logicielle.abrunetti.fr/";
    try {
        const response = await fetch(url);
        if (!response.ok) {
            throw new Error(`Erreur: ${response.status}`);
        }
        const text = await response.text();
        document.getElementById("fetch-content").textContent = text;
    } catch (error) {
        document.getElementById("fetch-content").textContent = "Erreur lors du chargement: " + error.message;
    }
}
</script>
</div>

<details><summary class="reponse" >Code source html / js avec la librairie `fetch`</summary>
<p>

```html
<div id="fetch-with-fetch">
<button id="fetch-btn" style="display: inline-block; padding: 10px 20px; background-color: #007bff; color: white; font-size: 16px; border-radius: 5px; cursor: pointer; text-align: center; font-family: Arial, sans-serif;" onclick="fetchPageFetch()">Charger la page avec le package fetch</button>
<div id="fetch-content" style="white-space: pre-wrap; border: 1px solid #000; padding: 10px; margin-top: 10px;"></div>

<script>
async function fetchPageFetch() {
    const url = "https://conception-logicielle.abrunetti.fr/";
    try {
        const response = await fetch(url);
        if (!response.ok) {
            throw new Error(`Erreur: ${response.status}`);
        }
        const text = await response.text();
        document.getElementById("fetch-content").textContent = text;
    } catch (error) {
        document.getElementById("fetch-content").textContent = "Erreur lors du chargement: " + error.message;
    }
}
</script>
</div>

```

</p></details>

Pour simplifier la gestion des requ√™tes HTTP et √©viter de devoir construire manuellement les URLs, ce cours recommande l‚Äôutilisation de Axios, une librairie qui facilite les requ√™tes, la gestion des erreurs et des param√®tres.
Voici un exemple d‚Äôutilisation pour r√©cup√©rer le contenu d‚Äôune page web statique.

<div id="axios">
<script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script> <!-- Import Axios -->

<button id="axios-btn" style="display: inline-block; padding: 10px 20px; background-color: #007bff; color: white; font-size: 16px; border-radius: 5px; cursor: pointer; text-align: center; font-family: Arial, sans-serif;" onclick="fetchPageAxios()">Charger la page avec le package axios</button>


<script>
        async function fetchPageAxios() {
            const url = "https://conception-logicielle.abrunetti.fr/";
            try {
                const response = await axios.get(url);
                document.getElementById("axios-content").textContent = response.data;
            } catch (error) {
                document.getElementById("axios-content").textContent = "Erreur lors du chargement: " + error.message;
            }
        }
</script>
<div id="axios-content" style="white-space: pre-wrap; border: 1px solid #000; padding: 10px; margin-top: 10px;"></div>
</div>


<details><summary class="reponse" >Code source html / js avec la librairie axios</summary>
<p>

```html
<div id="axios">
<script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script> <!-- Import Axios -->

<button id="axios-btn" style="display: inline-block; padding: 10px 20px; background-color: #007bff; color: white; font-size: 16px; border-radius: 5px; cursor: pointer; text-align: center; font-family: Arial, sans-serif;" onclick="fetchPageAxios()">Charger la page avec le package axios</button>


<script>
        async function fetchPageAxios() {
            const url = "https://conception-logicielle.abrunetti.fr/";
            try {
                const response = await axios.get(url);
                document.getElementById("axios-content").textContent = response.data;
            } catch (error) {
                document.getElementById("axios-content").textContent = "Erreur lors du chargement: " + error.message;
            }
        }
</script>
<div id="axios-content" style="white-space: pre-wrap; border: 1px solid #000; padding: 10px; margin-top: 10px;"></div>
</div>
```

</p></details>

> Remarque, c'est exactement la m√™me base de code pour la r√©cup√©ration de donn√©es depuis une API, ce qui d'ailleurs est plus fr√©quent en **JS**.


### Web Crawling: exemple avec `Selenium`

<img src="https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fznde9s4sx4iysia7doil.png">

Le web crawling est un autre mode de `webscraping`, l'objectif ici est d'utiliser un site web et de le parcourir comme un utilisateur a l'aide d'un script. Cela peut √™tre tr√®s utile lorsque vous d√©sirez extraire des donn√©es d'un site qui n√©cessite une authentification, ou que vous d√©sirez r√©aliser des sc√©narios plus complexes d'extractions.

> **Cela est √©galement tr√®s utile pour tester les fonctionnalit√©s c√¥t√© Frontend, a partir d'une url.**


Selenium est un outil qui permet d'executer des actions script√©es comme le parcours de pages sur des interfaces web, il est tr√®s utilis√© et poss√®de une int√©gration dans diff√©rents languages : **java**, **python**, **javascript**,.. 

Fonctionnellement, `Selenium` s'appuie sur les driver de navigateur pour lancer en t√¢che de fond ou en interactif un navigateur a partir duquel il va pouvoir interagir avec les pages web.

Pour l'installer il faut donc :
- l'installer avec pip : `pip3 install selenium`
- Installer un webdriver : Soit [firefox](https://github.com/mozilla/geckodriver/releases), soit [chrome](https://developer.chrome.com/docs/chromedriver/downloads?hl=fr)

> Pour les exemples, on utilise gecko, le webdriver de firefox

Les cas d'utilisation de s√©l√©nium est le parcours de page pour r√©cup√©rer des informations :

- **Un Exemple :** r√©cup√©ration des plateformes et projection sur lesquelles sont diffus√©es les films,voir https://github.com/conception-logicielle-ensai/exemples-cours/blob/main/cours-5/webscrapping/recuperation_donnees/selenium_exemple.py

- **Tests fonctionnels** De la m√™me mani√®re, on peut effectuer des tests de non regression et des tests d'int√©gration sur nos applications frontend et nos API avec selenium. Il suffit de v√©rifier qu'en allant a une adresse connue, on ait des √©l√©ments de r√©ponse attendus valides.


## Extraction de donn√©es HTML : `str`, regex et parsing

Une fois les pages r√©cup√©r√©es, les donn√©es **ne sont pas directement exploitables**, elles n√©cessitent un retraitement pour isoler l‚Äôinformation utile des √©l√©ments d‚Äôaffichage (balises, styles, scripts‚Ä¶).

Par exemple, on peut vouloir aggr√©ger les indicateurs r√©cup√©r√©s ou pr√©parer des donn√©es textuelles pour ensuite pouvoir entrainer un mod√®le sur ces donn√©es.

### Extraction simple avec les fonctions natives de `str`

On peut extraire des informations **manuellement** avec les fonctions natives Python sur les cha√Ænes de caract√®res (`str`) : `find`, `replace`, boucles, etc.

**Exemple : r√©cup√©rer les titres des parties de cours dans une page HTML**

```python
def recuperation_partie_summary(ligne):
    """
    R√©cup√®re le contenu d'une balise <summary> dans une ligne HTML
    """
    if "<summary>" in ligne:
        part = ligne.strip().replace("<summary>", "").replace("</summary>", "")
        return part
    return None

def extraction_grandes_parties_raw(html:str):
    """
    Extrait les titres des parties depuis la balise <nav role="navigation">.
    M√©thode basique, sans parsing sp√©cifique.
    """
    balise_contenant_parties = "<nav role=\"navigation\">"
    balise_finissant_parties = "</nav>"
    
    start_idx = html.find(balise_contenant_parties)
    end_idx = html.find(balise_finissant_parties, start_idx)
    
    if start_idx == -1 or end_idx == -1:
        return []
    
    interieur_navigation = html[start_idx:end_idx]
    parts = []
    
    for line in interieur_navigation.split("\n"):
        partie = recuperation_partie_summary(line)
        if partie is not None:
            parts.append(partie)
    
    return parts

# Exemple d‚Äôutilisation
extraction_grandes_parties_raw(html)
# ['üí¨ A propos', 'üóÇÔ∏è Projets', 'Cours 1 - architecture de base et √©volution', ...]
```

**Limites :**

* Fragile face aux **modifications de structure HTML** (ajout de classes, espaces, sauts de ligne‚Ä¶).
* Peu efficace pour des pages longues ou complexes.

### Extraction avec les expressions r√©guli√®res (Regex)

<img src="https://cms-assets.tutsplus.com/cdn-cgi/image/width=630/uploads/users/1251/posts/93367/image-upload/password_check_regex.jpg">

Les **expressions r√©guli√®res**, ou **regex** pour faire court, sont des **motifs** que l‚Äôon utilise pour **rechercher et manipuler du texte**. On parle souvent de `pattern matching`.
L‚Äôid√©e est de cr√©er un **mod√®le** qui va correspondre √† des ensembles coh√©rents de cha√Ænes de caract√®res.

**Pourquoi utiliser les regex ?**

* Identifier et extraire des donn√©es √† l‚Äôint√©rieur de balises HTML ou XML.
* Rechercher des occurrences de mots ou motifs sp√©cifiques dans de longues cha√Ænes de texte.
* Valider ou filtrer des informations (emails, num√©ros de t√©l√©phone, codes‚Ä¶).

> Les regex ne servent pas uniquement au web scraping : elles sont utiles dans tous types de traitement de texte.

#### Utiliser les regex avec Python

Python propose le module `re` dans sa biblioth√®que standard pour travailler avec les expressions r√©guli√®res.

```python
import re
```
#### Syntaxe de base

**Ancres**

| Ancres | Description  |
| ------ |------------------------------------------------------------------ |
| `^`    | D√©but de ligne. Correspond au d√©but d'une cha√Æne.                                                 |
| `$`    | Fin de ligne. Correspond √† la fin d'une cha√Æne.                                                   |
| `\b`   | Limite de mot. Correspond √† la position entre un caract√®re de mot (`\w`) et un caract√®re non-mot. |

**Symboles sp√©ciaux**

| Symbole sp√©cial | Description                                                        |
| --------------- | ------------------------------------------------------------------ |
| `.`             | Correspond √† n'importe quel caract√®re sauf un saut de ligne.       |
| `*`             | Correspond √† z√©ro ou plusieurs occurrences du caract√®re pr√©c√©dent. |
| `\d`            | Correspond √† un chiffre (√©quivalent √† `[0-9]`).                    |
| `\D`            | Tout caract√®re qui n'est pas un chiffre (`[^0-9]`).                |
| `\w`            | Caract√®re alphanum√©rique (`[a-zA-Z0-9_]`).                         |
| `\W`            | Tout caract√®re non-alphanum√©rique (`[^a-zA-Z0-9_]`).               |
| `\s`            | Caract√®re d‚Äôespacement (espace, tabulation, retour √† la ligne).    |
| `\S`            | Tout caract√®re qui n‚Äôest pas un espacement.                        |

**Exemples simples**

```text
^abc      ‚Üí "abc" au d√©but de la ligne
xyz$      ‚Üí "xyz" √† la fin de la ligne
\d{3}     ‚Üí trois chiffres cons√©cutifs
\w+       ‚Üí un ou plusieurs caract√®res alphanum√©riques
^toto.*   ‚Üí toute ligne qui commence par "toto"
```

#### Groupes de capture

Les **groupes de capture** permettent d‚Äôisoler **une portion sp√©cifique** d‚Äôun motif. Ils sont d√©finis avec des **parenth√®ses `( )`**.

| Expression r√©guli√®re | Description                                         |
| -------------------- | --------------------------------------------------- |
| `(.*)`               | Capture **toute la cha√Æne**                         |
| `<li>(.*?)</li>`     | Capture **tout ce qui est entre `<li>` et `</li>`** |

**R√©cup√©ration des groupes**

* `\0` ‚Üí correspond **au match complet**
* `\1` ‚Üí premier groupe captur√©
* `\2` ‚Üí deuxi√®me groupe captur√©, etc.

#### Exemple concret en Python

```python
import re

texte = "<li>Cours 1 - architecture</li>"

# Deux groupes : le tag ouvrant et le contenu
pattern = r"(<li>)(.*?)</li>"

match = re.search(pattern, texte)
if match:
    print("Groupe 0 :", match.group(0))  # <li>Cours 1 - architecture</li>
    print("Groupe 1 :", match.group(1))  # <li>
    print("Groupe 2 :", match.group(2))  # Cours 1 - architecture
```

#### Exemple pratique : extraire des titres de cours

```python
import re

def extract_with_regex(html, pattern):
    """
    R√©cup√©ration de toutes les donn√©es correspondant √† un pattern regex
    """
    matches = re.findall(pattern, html, re.DOTALL)
    return [match.strip() for match in matches]

# Tous les titres
pattern_titre_cours = r"<summary>(.*?)</summary>"

# Titres commen√ßant par A
pattern_titre_cours_commencant_par_a = r"<summary>(A.*?)</summary>"

titre_cours = extract_with_regex(html, pattern_titre_cours)
titre_cours_commencant_par_a = extract_with_regex(html, pattern_titre_cours_commencant_par_a)

print("Tous les titres :", titre_cours)
print("Titres commen√ßant par A :", titre_cours_commencant_par_a)
```

### Parsing HTML avec `BeautifulSoup`

<img src="https://oxylabs.io/_next/image?url=https%3A%2F%2Foxylabs.io%2Foxylabs-web%2FZpBvKB5LeNNTxEoc_NWNiMmRiN2MtNzlkNC00OGIxLTg4NGUtZjZlMWY1ZWQ4NmMz_using-python-and-beautiful-soup-to-parse-data-intro-tutorial2x-3.png%3Fauto%3Dformat%2Ccompress&w=3840&q=75=">

Pour **un parsing plus stable et robuste**, on utilise **BeautifulSoup**, une librairie externe :

```bash
pip3 install beautifulsoup4
```

```python
from bs4 import BeautifulSoup
import requests

url = "https://conception-logicielle.abrunetti.fr/cours-2024/"
res = requests.get(url)
html = res.text
soup = BeautifulSoup(html, "html.parser")
```

BeautifulSoup permet de naviguer dans l‚Äôarborescence HTML et d‚Äôextraire facilement les √©l√©ments souhait√©s.

#### S√©lection avec CSS ou structure HTML

Supposons qu'on a ce fichier HTML :

```html
<html>
  <head>
    <title>Exemple de page</title>
  </head>
  <body>
    <div class="content">
      <h2>Introduction</h2>
      <h2>Chapitre 1 - Concepts</h2>
      <h2>Chapitre 2 - Applications</h2>
      <p>Voici du texte dans la section content.</p>
    </div>

    <div class="article-body">
      <p>Paragraphe principal de l'article.</p>
    </div>

    <nav>
      <ul>
        <li><a href="page1.html">Page 1</a></li>
        <li><a href="page2.html">Page 2</a></li>
        <li><a href="https://externe.com">Lien externe</a></li>
      </ul>
    </nav>
  </body>
</html>
```

- Pour r√©cup√©rer tous les `<h2>` √† l'int√©rieur de `<div class="content">` :

```python
from bs4 import BeautifulSoup

html = """ ...HTML ci-dessus... """
soup = BeautifulSoup(html, "html.parser")

# S√©lection avec CSS selector
h2_titles = soup.select("div.content h2")
print([h2.text for h2 in h2_titles]) 
# ['Introduction', 'Chapitre 1 - Concepts', 'Chapitre 2 - Applications']
```

- Pour r√©cup√©rer tous les liens `<a>` de la page

```python
links = [a["href"] for a in soup.find_all("a", href=True)]
print(links)
# ['page1.html', 'page2.html', 'https://externe.com']
```


- Pour r√©cup√©rer un √©l√©ment par balise et attribut

```python
article = soup.find("div", class_="article-body")
print(article.text.strip())
# 'Paragraphe principal de l\'article.'
```
> Plus de documentation, dans [la documentation oficielle](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

> [!TIP]+ Pour aller plus loin
> [Documentation officielle : BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

> [!TIP]+ Pour aller plus loin
>  Un autre outil tr√®s complet pour des  projets python : [`scrapy`](https://scrapy.org/)


## Ressources externes (pour aller plus loin)

- Tr√®s bonne formation par Antoine Palazzo sur le webscrapping : https://inseefrlab.github.io/formation-webscraping
- Cours de datascience ENSAE, cours de web scrapping par Lino Galliana: https://pythonds.linogaliana.fr/content/manipulation/04a_webscraping_TP.html

<div id="secret" hidden>

Exercice secret :

Dans cette page est cach√© un contenu qui contient diff√©rentes valeurs. R√©cup√©rez le contenu de cet √©l√©ment cach√© et sommez les valeurs. 
1. A l'aide de votre navigateur et de votre console d√©veloppeur, trouver l'`id` du contenu cach√© sur cette page
Quel est le r√©sultat de cette somme ? 
- 2595677
- 2609777
- 2609997

    <ul>
    <li>1290908</li>
    <li>1290918</li>
    <li>12908</li>
    <li>919</li>
    <li>21</li>
    <li>3</li>
    </ul>
</div>