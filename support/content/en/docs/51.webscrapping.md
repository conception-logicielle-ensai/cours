---
url: /docs/webscrapping/
title: üì§ Webscrapping et Client HTTP.
---

<img src="https://d1pnnwteuly8z3.cloudfront.net/images/4d5bf260-c3d0-4f21-b718-8ede8d4ca716/febf9de6-8a5a-4055-b274-e685485496f5.jpeg" />

Le web scraping d√©signe les techniques d‚Äôextraction du contenu des sites internet. C'est une pratique que l'on envisage lorsque l'on a acc√®s a des donn√©es utiles publiquement mais qu'il ne nous est pas fourni de fichiers produits ou d'API pour les traiter. 

Elle s'appuie donc sur l'utilisation de client HTTP pour traiter des donn√©es non formatt√©es pour l'utilisation, typiquement des pages web mises a disposition aux utilisateurs au format `HTML`.

> Le terme fait g√©n√©ralement r√©f√©rence √† l‚Äôusage de bots pour collecter ces contenus automatiquement.

Le webscraping peut s'appr√©cier au travers de processus de type `ETL` (Extract Transform Load) :
- Extract : on r√©alise l'extraction des donn√©es brutes a partir de pages web.
- Transform : on effectue un traitement a partir des donn√©es brutes pour les rendre exploitables. `parsing`
- Load : on les sauvegarde dans un fichier, une base de donn√©es ou autre.

> Le plan de ce cours s'articulera selon ces parties avec une partie sur la persistence en 3√®me partie de ce cours.

> **Comme pour les autres parties, les exemples sont disponibles ici : [https://github.com/conception-logicielle-ensai/exemples-cours/tree/main/cours-5/webscrapping](https://github.com/conception-logicielle-ensai/exemples-cours/tree/main/cours-5/webscrapping)**

## Exemples d'utilisation

- Effectuer des suivi de prix : March√© / Bourse / Site d'annonces / Comparateur de prix - C'est le cas par exemple a l'INSEE dans la section **IPC**.
- R√©cup√©ration de donn√©es textuelles massives : commentaires, tweets, .. - lecture des avis googles pour 
- R√©cup√©ration de listes de contact : Annuaires, pages jaunes, pages blanches - Base de sondage entreprises via pages jaunes
- Aggr√©gation de statistiques d√©j√† calcul√©es : google trends, ..

## ü©∂ Webscrapping et l√©galit√© : existence d'une `zone grise`

Le webscraping est l'une des pratiques les plus courantes pour la r√©cup√©ration de donn√©es sur le web. Il peut √™tre utilis√© pour r√©cup√©rer des donn√©es publiquement accessibles, mais ces donn√©es sont parfois prot√©g√©es par le `droit d'auteur` ou pour les `donn√©es personnelles`. C'est ce qui explique qu'il y a l√† une zone grise.

Droit des donn√©es :

- En Europe, actuellement il y a la [General Data Protection Regulation (GDPR) (ou RGPD) ](https://gdpr-info.eu/) qui prot√®gent les donn√©es personnelles mais ne pr√©cisent pas le traitement dans le cas d'usage du webscraping.

- Aux √©tats unis : [California Privacy Rights Act (CPRA)](https://thecpra.org/), c'est une loi en californie, mais rien n'existe c√¥t√© f√©d√©ral sur le droit des donn√©es.
> Il est donc pr√©f√©rable d'utiliser des APIs au maximum pour la r√©cup√©ration de donn√©es lorsque celles-ci sont disponibles.



## üåë R√©cup√©ration des donn√©es : Client HTTP

Les clients http sont n√©cessaires pour la r√©cup√©ration des donn√©es expos√©es sur les sites. Il en existe de diff√©rents types.

### Client en ligne de commande

Les outils comme **Wget** et **cURL** permettent d'envoyer des requ√™tes HTTP directement depuis le terminal.

- cURL : Supporte plusieurs protocoles (HTTP, FTP, etc.), permet d'envoyer des requ√™tes GET, POST, etc.
- Wget : Principalement utilis√© pour t√©l√©charger des fichiers depuis le web.

> Ils permettent de scripter des requ√™tes et sont disponibles sur la plupart des OS.

> **Remarque : Nous vous avons pr√©sent√© curl dans la session pr√©c√©dente. Il est tr√®s utile puisqu'il est commun d'√©changer des commandes curl entre des √©quipes de d√©veloppeur (DEV) ou des √©quipes de maintenance d'infrastructure (OPS)**

### Clients utilitaires

Il existe des clients utilitaires comme **Insomnia** et **Postman** offrent une interface graphique pour tester des API REST. 

- `Postman` : Permet d'envoyer des requ√™tes HTTP, d'automatiser des tests et de documenter des API.
- `Insomnia` : Similaire √† `Postman`, ax√© sur la simplicit√© et l‚Äôexp√©rience utilisateur.


<div class="alert alert-info">
  <strong> Pour aller plus loin </strong> <br/>

Lien vers les sites pour t√©l√©charger/get started avec ces clients http : [Postman](https://www.postman.com/) et  [Insomnia](https://insomnia.rest/)

</div>



> Ils peuvent √™tre tr√®s pratiques si vous travaillez avec des coll√®gues ne maitrisant pas python puisque vous pouvez leur partager vos scripts.

### Navigateur et utilisation des outils de d√©veloppement

<img src="https://firefox-source-docs.mozilla.org/_images/pageinspector.png" />

Les navigateurs sont des clients HTTP tr√®s adapt√©s pour effectuer des requ√™tes HTTP. Ils proposent par ailleurs une interface de d√©vtools directement incorpor√©e.

Ils proposent :
- Un onglet R√©seau : Cela permet de traquer les requ√™tes HTTP effectu√©es par le navigateur. Cela peut √™tre utile pour les stocker ou les reproduire via script.
- Un onglet debug: interface debug c√¥t√© client (nos pages executant du javascript cela permet de comprendre un bug d'affichage par ex)
- Une console: elle permet d'executer du javascript sur la page
- Un inspecteur : permet de scanner les √©l√©ments et de r√©cup√©rer des informations sur celles ci.

**...Et d'autres fonctionnalit√©s..**

Consultons ensemble cette documentation : 

- [https://firefox-source-docs.mozilla.org/devtools-user/](https://firefox-source-docs.mozilla.org/devtools-user/)

<div class="alert alert-info">
  <strong> Pour aller plus loin </strong> <br/>

Lien vers une vid√©o pr√©sentant les outils de d√©veloppements chrome : [https://www.youtube.com/watch?v=BrsyIyYSP1c](https://www.youtube.com/watch?v=BrsyIyYSP1c)
</div>


### Utilisation de requests
Pour effectuer des requ√™tes http on utilisera a nouveau la librairie client http `requests`.

Les requ√™tes seront cette fois effectu√©es pour la r√©cup√©ration de pages web, pour r√©cup√©rer des fichiers HTML bruts, et donc on privil√©giera la r√©cup√©ration du text dans les r√©ponses : 

```py
import requests

URL_COURS = "https://conception-logicielle.abrunetti.fr/"
response = requests.get(url)
print(response.text)  # Affiche le contenu HTML de la page
```

### Utilisation d'un script dans une page HTML (Javascript)

En javascript il existe de nombreux client HTTP, nous privil√©gierons l'usage d'axios en seconde partie du cours, mais le client http "de base" est **fetch** :

Observez plut√¥t en r√©cup√©rant le code source de la **page d'accueil** du cours :

<div id="fetch-with-fetch">
<button id="fetch-btn" style="display: inline-block; padding: 10px 20px; background-color: #007bff; color: white; font-size: 16px; border-radius: 5px; cursor: pointer; text-align: center; font-family: Arial, sans-serif;" onclick="fetchPageFetch()">Charger la page avec le package fetch</button>
<div id="fetch-content" style="white-space: pre-wrap; border: 1px solid #000; padding: 10px; margin-top: 10px;"></div>

<script>
async function fetchPageFetch() {
    const url = "https://conception-logicielle.abrunetti.fr/";
    try {
        const response = await fetch(url);
        if (!response.ok) {
            throw new Error(`Erreur: ${response.status}`);
        }
        const text = await response.text();
        document.getElementById("fetch-content").textContent = text;
    } catch (error) {
        document.getElementById("fetch-content").textContent = "Erreur lors du chargement: " + error.message;
    }
}
</script>
</div>

<details><summary class="reponse" >Code source html / js avec la librairie fetch</summary>
<p>

```html
<div id="fetch-with-fetch">
<button id="fetch-btn" style="display: inline-block; padding: 10px 20px; background-color: #007bff; color: white; font-size: 16px; border-radius: 5px; cursor: pointer; text-align: center; font-family: Arial, sans-serif;" onclick="fetchPageFetch()">Charger la page avec le package fetch</button>
<div id="fetch-content" style="white-space: pre-wrap; border: 1px solid #000; padding: 10px; margin-top: 10px;"></div>

<script>
async function fetchPageFetch() {
    const url = "https://conception-logicielle.abrunetti.fr/";
    try {
        const response = await fetch(url);
        if (!response.ok) {
            throw new Error(`Erreur: ${response.status}`);
        }
        const text = await response.text();
        document.getElementById("fetch-content").textContent = text;
    } catch (error) {
        document.getElementById("fetch-content").textContent = "Erreur lors du chargement: " + error.message;
    }
}
</script>
</div>

```

</p></details>

Pour une meilleure gestion des client http, plut√¥t que de devoir reparam√©trer les urls, on pr√©conise un usage d'axios dans ce cours, voici un exemple sur une page statique.

<div id="axios">
<script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script> <!-- Import Axios -->

<button id="axios-btn" style="display: inline-block; padding: 10px 20px; background-color: #007bff; color: white; font-size: 16px; border-radius: 5px; cursor: pointer; text-align: center; font-family: Arial, sans-serif;" onclick="fetchPageAxios()">Charger la page avec le package axios</button>


<script>
        async function fetchPageAxios() {
            const url = "https://conception-logicielle.abrunetti.fr/";
            try {
                const response = await axios.get(url);
                document.getElementById("axios-content").textContent = response.data;
            } catch (error) {
                document.getElementById("axios-content").textContent = "Erreur lors du chargement: " + error.message;
            }
        }
</script>
<div id="axios-content" style="white-space: pre-wrap; border: 1px solid #000; padding: 10px; margin-top: 10px;"></div>
</div>


<details><summary class="reponse" >Code source html / js avec la librairie axios</summary>
<p>

```html
<div id="axios">
<script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script> <!-- Import Axios -->

<button id="axios-btn" style="display: inline-block; padding: 10px 20px; background-color: #007bff; color: white; font-size: 16px; border-radius: 5px; cursor: pointer; text-align: center; font-family: Arial, sans-serif;" onclick="fetchPageAxios()">Charger la page avec le package axios</button>


<script>
        async function fetchPageAxios() {
            const url = "https://conception-logicielle.abrunetti.fr/";
            try {
                const response = await axios.get(url);
                document.getElementById("axios-content").textContent = response.data;
            } catch (error) {
                document.getElementById("axios-content").textContent = "Erreur lors du chargement: " + error.message;
            }
        }
</script>
<div id="axios-content" style="white-space: pre-wrap; border: 1px solid #000; padding: 10px; margin-top: 10px;"></div>
</div>
```

</p></details>

> Remarque, c'est exactement la m√™me base de code pour la r√©cup√©ration de donn√©es depuis une API, ce qui d'ailleurs est plus fr√©quent en **JS**.

### üï∑ Web Crawling: exemple avec `Selenium`
<img src="https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fznde9s4sx4iysia7doil.png">

Le web crawling est un autre mode de `webscraping`, l'objectif ici est d'utiliser un site web et de le parcourir comme un utilisateur a l'aide d'un script. Cela peut √™tre tr√®s utile lorsque vous d√©sirez extraire des donn√©es d'un site qui n√©cessite une authentification, ou que vous d√©sirez r√©aliser des sc√©narios plus complexes d'extractions.

> **Cela est √©galement tr√®s utile pour tester les fonctionnalit√©s c√¥t√© Frontend, a partir d'une url.**


Selenium est un outil qui permet d'executer des actions script√©es comme le parcours de pages sur des interfaces web, il est tr√®s utilis√© et poss√®de une int√©gration dans diff√©rents languages : **java**, **python**, **javascript**,.. 

Fonctionnellement, `Selenium` s'appuie sur les driver de navigateur pour lancer en t√¢che de fond ou en interactif un navigateur a partir duquel il va pouvoir interagir avec les pages web.

Pour l'installer il faut donc :
- l'installer avec pip : `pip3 install selenium`
- Installer un webdriver : Soit [firefox](https://github.com/mozilla/geckodriver/releases), soit [chrome](https://developer.chrome.com/docs/chromedriver/downloads?hl=fr)

> Pour les exemples, on utilise gecko, le webdriver de firefox

Les cas d'utilisation de s√©l√©nium est le parcours de page pour r√©cup√©rer des informations :

- **Un Exemple :** r√©cup√©ration des plateformes et projection sur lesquelles sont diffus√©es les films,voir https://github.com/conception-logicielle-ensai/exemples-cours/blob/main/cours-5/webscrapping/recuperation_donnees/selenium_exemple.py

- **Tests fonctionnels** De la m√™me mani√®re, on peut effectuer des tests de non regression et des tests d'int√©gration sur nos applications frontend et nos API avec selenium. Il suffit de v√©rifier qu'en allant a une adresse connue, on ait des √©l√©ments de r√©ponse attendus valides.


## Manipulation des donn√©es

Une fois les pages r√©cup√©r√©es, il faut les traiter pour en r√©cup√©rer des donn√©es exploitables pour des traitements internes a nos applicatifs.

Par exemple, on peut vouloir aggr√©ger les indicateurs r√©cup√©r√©s ou pr√©parer des donn√©es textuelles pour ensuite pouvoir entrainer un mod√®le sur ces donn√©es.

### üîé Regex, Expressions r√©guli√®res : Isoler et capturer des donn√©es textuelles

<img src="https://cms-assets.tutsplus.com/cdn-cgi/image/width=630/uploads/users/1251/posts/93367/image-upload/password_check_regex.jpg">

Les expressions **r√©guli√®res**, ou **regex** pour faire court, sont des motifs que vous pouvez utiliser pour rechercher du texte dans une cha√Æne de caract√®res. 
On parle assez souvent de `pattern` `matching`. On va donc ici √©laborer des patterns pour r√©cup√©rer les ensembles coh√©rents de chaine de caract√®res qui respectent ce pattern.

Quel int√™ret ? Ici l'on va parcourir des balises html diverses `div` `ul` et l'on va vouloir par exemple r√©cup√©rer les m√©tadonn√©es contenues dans ces balises.

Python prend en charge les expressions r√©guli√®res gr√¢ce au module `re` d√©j√† pr√©sent dans sa biblioth√®que standard.

> C'est un concept qui est pr√©sent dans la plupart des languages. Il est donc r√©utilisable lors de probl√©matiques de traitement de donn√©es au format `str`

#### Syntaxe 

| Ancres | Description                                               |
|--------|-----------------------------------------------------------|
| ^      | D√©but de ligne. Correspond au d√©but d'une cha√Æne de caract√®res. |
| $      | Fin de ligne. Correspond √† la fin d'une cha√Æne de caract√®res. |
| \b     | Limite de mot. Correspond √† la position entre un caract√®re de mot (\w) et un caract√®re qui n'est pas un caract√®re de mot. |

| Symbole sp√©cial | Description                                               |
|-----------------|-----------------------------------------------------------|
| .               | Correspond √† n'importe quel caract√®re, sauf un saut de ligne. |
| *               | Correspond √† z√©ro ou plusieurs occurrences du caract√®re pr√©c√©dent. |
| \d              | Correspond √† un chiffre. √âquivalent √† [0-9].              |
| \D              | Correspond √† tout caract√®re qui n'est pas un chiffre. √âquivalent √† [^0-9]. |
| \w              | Correspond √† un caract√®re alphanum√©rique (lettres, chiffres, soulign√©). √âquivalent √† [a-zA-Z0-9_]. |
| \W              | Correspond √† tout caract√®re qui n'est pas alphanum√©rique. √âquivalent √† [^a-zA-Z0-9_]. |
| \s              | Correspond √† un caract√®re d'espacement (espace, tabulation, retour √† la ligne). |
| \S              | Correspond √† tout caract√®re qui n'est pas un caract√®re d'espacement. |

Exemples d'utilisation : 

- **^abc** : Correspond √† la cha√Æne "abc" au d√©but de la ligne.
- **xyz$** : Correspond √† la cha√Æne "xyz" √† la fin de la ligne.
- **\d{3}** : Correspond √† trois chiffres cons√©cutifs.
- **\w+** : Correspond √† un ou plusieurs caract√®res alphanum√©riques.
- **^toto.*** r√©cup√®re toute la ligne si elle contient toto au d√©but
#### Groupes de Capture :
Les groupes de capture sont utilis√©s pour capturer une partie sp√©cifique d'une correspondance d'expression r√©guli√®re. Ils sont d√©limit√©s par des parenth√®ses.

| Expression r√©guli√®re | Description                                |
|----------------------|--------------------------------------------|
| (.*)                 | Capture toute la chaine                    |
| `<li>(.*?)</li>`     | Capture tout ce qui est entre la balise li |

On peut ensuite utiliser les groupes de capture avec `\1` ou `\0`

**Exemple pour r√©cup√©rer le nombre de parties du cours envoy√©es sur le site du cours:**
```python
import re
def extract_with_regex(html):
    """
    Recuperation de toutes les donn√©es qui sont dans les balises summary a l'aide du groupe de capture
    """
    pattern = r"<summary>(.*?)</summary>"
    matches = re.findall(pattern, html, re.DOTALL)
    return [match.strip() for match in matches]

parties = extract_with_regex(html)
print(parties)
# ['üí¨ A propos', 
#   'üóÇÔ∏è Projets', 
# 'Cours 1 - architecture de base et √©volution',
#  'Cours 2 - qualification et bonnes pratiques', 
#  'Cours 3 - configuration et portabilit√©',
#  'Cours 4 - API webservice et d√©veloppement web',
#  'Cours 5 - Interfaces Homme-Machine et donn√©es',
#  'Cours 6 - D√©ploiement et h√©bergement applicatifs'] 
```

<details><summary class="reponse" ><b>Ce qu'il faut d√©sormais limiter </b></summary>
<p>

#### Fonctions natives de str
La r√©cup√©ration des donn√©es issues d'un site au format `html` est possible par diff√©rents outils de type client `HTTP`. Ces donn√©es ne sont pas exploitables telles quelles, elle n√©cessitent a minima un retraitement par rapport a tous les √©l√©ments d'affichage inutiles pour l'exploitation des donn√©es.

Ce retraitement peut se fait de mani√®re manuelle dans les str, avec les fonctions `find`, des boucles .. Mais cela n'est pas efficient et n'est pas adapt√© a des changements de casse dans le site (saut de ligne, espace, ajout d'une classe sur les summary..)

**Exemple pour r√©cup√©rer le nombre de parties du cours envoy√©es sur le site du cours:**
```python
def recuperation_partie_summary(ligne):
    """
    Recup√®re l'int√©rieur de la partie summary pour le html qui contient le nom de la partie
    """
    if "<summary>" in ligne:
            part = ligne.strip().replace("<summary>", "").replace("</summary>", "")
            return part
    return None
def extraction_grandes_parties_raw(html:str):
    """
    Fonction qui extrait les sous parties de la page dans la balise navigation
    D√©velopp√©e sans fonctionnalit√©s de parsing
    """
    balise_contenant_parties = "<nav role=\"navigation\">"
    balise_finissant_parties= "</nav>"
    
    start_idx = html.find(balise_contenant_parties)
    end_idx = html.find(balise_finissant_parties, start_idx)
    
    if start_idx == -1 or end_idx == -1:
        return []
    
    interieur_navigation = html[start_idx:end_idx]
    parts = []
    
    for line in interieur_navigation.split("\n"):
        partie = recuperation_partie_summary(line)
        if partie != None:
             parts.append(partie)
    
    return parts


extraction_grandes_parties_raw(html)
# ['üí¨ A propos', 
#   'üóÇÔ∏è Projets', 
# 'Cours 1 - architecture de base et √©volution',
#  'Cours 2 - qualification et bonnes pratiques', 
#  'Cours 3 - configuration et portabilit√©',
#  'Cours 4 - API webservice et d√©veloppement web',
#  'Cours 5 - Interfaces Homme-Machine et donn√©es',
#  'Cours 6 - D√©ploiement et h√©bergement applicatifs'] 
```

</details>

<div class="alert alert-info">
  <strong> Pour aller plus loin </strong> <br/>
    Un jeu qui vous permet d'apprendre les regex et de pratiquer : <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">https://regexcrossword.com/</a>
</div>


### üî™ Parsing - Parser Html avec beautiful soup

<img src="https://oxylabs.io/_next/image?url=https%3A%2F%2Foxylabs.io%2Foxylabs-web%2FZpBvKB5LeNNTxEoc_NWNiMmRiN2MtNzlkNC00OGIxLTg4NGUtZjZlMWY1ZWQ4NmMz_using-python-and-beautiful-soup-to-parse-data-intro-tutorial2x-3.png%3Fauto%3Dformat%2Ccompress&w=3840&q=75=">

Beautiful Soup permet d'encapsuler l'arborescence des √©l√©ments html et xml dans un objet afin de pouvoir assez facilement le parcourir.

C'est une librairie externe on doit l'installer avec `pip` : `pip3 install beautifulsoup4`

```python
from bs4 import BeautifulSoup
import requests

url = "https://conception-logicielle.abrunetti.fr/cours-2024/"
res = requests.get(url)
html = res.text
soup = BeautifulSoup(html, "html.parser")
```

Il se base sur les selecteurs css pour la r√©cup√©ration de donn√©es : 

*Exemple on souhaite r√©cup√©rer tous les titres `<h2>` √† l'int√©rieur d'un `<div class="content">`*


Cela donne en python:
```python
soup.select("div.content h2")
```

On peut √©galement r√©cup√©rer en cherchant dans la structure:

*La balise `<a>` representant un lien dans la page*
```python
# Oneliner avec l'operateur list comprehension en python
links = [a["href"] for a in soup.find_all("a", href=True)]
```

Ou chercher par des attributs sur l'element HTML: 
```python
soup.find("div", class_="article-body")
```

<details><summary class="reponse" ><b>Exemple  : r√©cup√©ration des grandes parties du cours </b></summary>
<p>

```python
def extract_parties_avec_beautifulsoup(html):
    soup = BeautifulSoup(html, "html.parser")
    return [summary.text.strip() for summary in soup.find_all("summary")]

parties = extract_parties_avec_beautifulsoup(html)
print(parties)
# ['üí¨ A propos', 
#   'üóÇÔ∏è Projets', 
# 'Cours 1 - architecture de base et √©volution',
#  'Cours 2 - qualification et bonnes pratiques', 
#  'Cours 3 - configuration et portabilit√©',
#  'Cours 4 - API webservice et d√©veloppement web',
#  'Cours 5 - Interfaces Homme-Machine et donn√©es',
#  'Cours 6 - D√©ploiement et h√©bergement applicatifs'] 
```

</p>
</details>

**Cela permet au global un meilleur parsing et une meilleure stabilit√© dans le traitement des fichiers html, c'est ce que l'on privil√©giera pour le webscraping**

<div class="alert alert-info">
  <strong> Pour aller plus loin </strong> <br/>
    Doc officielle : <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a>
</div>
<div class="alert alert-info">
  <strong> Pour aller plus loin </strong> <br/>
    Un autre outil tr√®s complet pour des  projets python : <a href="https://scrapy.org/">scrapy </a>
</div>

## üîê Protection contre le webscrapping : `Humaniser nos processus`.

La r√©cup√©ration de donn√©es en utilisant du webscrapping peut s'assimiler √† une attaque informatique de type `DDOS` (Denial Of Service).

Ainsi certains sites ont mis en oeuvre des solutions pour se prot√©ger contre le DDOS et le webscraping **abusif**. 
Ces solutions reposent sur les principes suivants : 

- Les sites bloquent des requ√™tes r√©p√©t√©es sur des intervalles de temps trop proches venant d'une m√™me IP (~m√™me machine)
- Les sites modifient contenu des balises html pour emp√™cher l'automatisation
- Cr√©ation de Honeypot üçØ : liens invisibles que seul un bot "cliquerait" pour bloquer les bots/botteurs.
- Authentification exig√©e au bout d'un certain nombre d'usages.

Ils proposent donc diff√©rentes guidelines g√©n√©rales pour les utilisateurs qui souhaitent webscraper : 
- Les sites pr√©cisent ce qu'ils permettent aux robots sur un endpoint particulier le endpoint `robots.txt`, exemple : [https://www.google.com/robots.txt](https://www.google.com/robots.txt)
- Il faut en g√©n√©ral essayer d'espacer un minimum les requ√™tes, il y a en g√©n√©ral des couches r√©seau "Anti DDOS" qui bloquent les requ√™tes venant d'une m√™me IP dans des intervalles de temps ress√©rr√©s.
- Eviter d'effectuer des requ√™tes dans des p√©riodes d'usage intensif des services r√©cup√©r√©s. Par exemple pour une administration fran√ßaise, privil√©gier d'effectuer nos commandes de webscraping pendant la nuit via la planification du traitement.



## Ressources externes (pour aller plus loin)

- Tr√®s bonne formation par Antoine Palazzo sur le webscrapping : https://inseefrlab.github.io/formation-webscraping
- Cours de datascience ENSAE, cours de web scrapping par Lino Galliana: https://pythonds.linogaliana.fr/content/manipulation/04a_webscraping_TP.html

<div id="secret" hidden>

Exercice secret :

Dans cette page est cach√© un contenu qui contient diff√©rentes valeurs. R√©cup√©rez le contenu de cet √©l√©ment cach√© et sommez les valeurs. 
1. A l'aide de votre navigateur et de votre console d√©veloppeur, trouver l'`id` du contenu cach√© sur cette page
Quel est le r√©sultat de cette somme ? 
- 2595677
- 2609777
- 2609997

    <ul>
    <li>1290908</li>
    <li>1290918</li>
    <li>12908</li>
    <li>919</li>
    <li>21</li>
    <li>3</li>
    </ul>
</div>