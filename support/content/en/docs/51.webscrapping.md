---
url: /docs/webscrapping/
title: üì§ Webscrapping et Client HTTP.
---
Le web scraping d√©signe les techniques d‚Äôextraction du contenu des sites internet. Cette une pratique que l'on envisage lorsque l'on a acc√®s a des donn√©es utiles publiquement mais qu'il ne nous est pas fourni de fichiers produits ou d'API pour les traiter. 

Elle s'appuie donc sur l'utilisation de client HTTP pour traiter des donn√©es non formatt√©es pour l'utilisation, typiquement des pages web mises a disposition aux utilisateurs au format `HTML`.

> Le terme fait g√©n√©ralement r√©f√©rence √† l‚Äôusage de bots pour collecter ces contenus automatiquement.

Le webscraping peut s'appr√©cier au travers de processus de type `ETL` (Extract Transform Load) :
- Extract : on r√©alise l'extraction des donn√©es brutes a partir de pages web.
- Transform : on effectue un traitement a partir des donn√©es brutes pour les rendre exploitables. `parsing`
- Load : on les sauvegarde dans un fichier, une base de donn√©es ou autre.

> Le plan de ce cours s'articulera selon ces parties avec une partie sur la persistence en 3√®me partie de ce cours.


## Exemples d'utilisation

- Effectuer des suivi de prix : March√© / Bourse / Site d'annonces / Comparateur de prix => C'est le cas par exemple a l'INSEE dans la section **IPC**.
- R√©cup√©ration de donn√©es textuelles massives : commentaires, tweets, ..
- R√©cup√©ration de listes de contact : Annuaires, pages jaunes, pages blanches => Base de sondage entreprise
- Aggr√©gation de statistiques d√©j√† calcul√©es : google trends, ..

## Webscrapping et l√©galit√© : existence d'une `zone grise`

Le webscraping est l'une des pratiques les plus courantes pour la r√©cup√©ration de donn√©es sur le web. Il peut √™tre utilis√© pour r√©cup√©rer des donn√©es publiquement accessibles, mais ces donn√©es sont parfois prot√©g√©es par le `droit d'auteur` ou pour les `donn√©es personnelles`. C'est ce qui explique qu'il y a l√† une zone grise.

Droit des donn√©es :

- En Europe, actuellement il y a la [General Data Protection Regulation (GDPR) (ou RGPD) ](https://gdpr-info.eu/) qui prot√®gent les donn√©es personnelles mais ne pr√©cisent pas le traitement dans le cas d'usage du webscraping.

- Aux √©tats unis : [California Privacy Rights Act (CPRA)](https://thecpra.org/), c'est une loi en californie, mais rien n'existe c√¥t√© f√©d√©ral sur le droit des donn√©es.
> Il est donc pr√©f√©rable d'utiliser des APIs au maximum pour la r√©cup√©ration de donn√©es lorsque celles-ci sont disponibles.

## R√©cup√©ration des donn√©es : Client HTTP

Les clients http sont n√©cessaires pour la r√©cup√©ration des donn√©es expos√©es sur les sites.

### Navigateur et utilisation des outils de d√©veloppement

### Utilisation de requests
Pour effectuer des requ√™tes http on utilisera a nouveau la librairie client http `requests`.

Les requ√™tes seront cette fois effectu√©es pour la r√©cup√©ration de pages web, pour r√©cup√©rer des fichiers HTML bruts, et donc on privil√©giera la r√©cup√©ration du text dans les r√©ponses : 

```py
import requests

url = "https://conception-logicielle.abrunetti.fr/"
response = requests.get(url)
print(response.text)  # Affiche le contenu HTML de la page
```

### Utilisation d'un script dans une page HTML (Javascript)

<button onclick="fetchPage()">Charger la page</button>
<div id="content" style="white-space: pre-wrap; border: 1px solid #000; padding: 10px; margin-top: 10px;"></div>

<script>
        async function fetchPage() {
            const url = "https://conception-logicielle.abrunetti.fr/"; // Remplace par l'URL souhait√©e
            try {
                const response = await fetch(url);
                if (!response.ok) {
                    throw new Error(`Erreur: ${response.status}`);
                }
                const text = await response.text();
                document.getElementById("content").textContent = text;
            } catch (error) {
                document.getElementById("content").textContent = "Erreur lors du chargement: " + error.message;
            }
        }
</script>

```html
<button onclick="fetchPage()">Charger la page</button>
<div id="content" style="white-space: pre-wrap; border: 1px solid #000; padding: 10px; margin-top: 10px;"></div>

<script>
        async function fetchPage() {
            const url = "https://example.org"; // Remplace par l'URL souhait√©e
            try {
                const response = await fetch(url);
                if (!response.ok) {
                    throw new Error(`Erreur: ${response.status}`);
                }
                const text = await response.text();
                document.getElementById("content").textContent = text;
            } catch (error) {
                document.getElementById("content").textContent = "Erreur lors du chargement: " + error.message;
            }
        }
</script>
```
## Web Crawling: exemple avec `Selenium`
<img src="/img/webservice/selenium.webp">

Le web crawling est un autre mode de webscraping, l'objectif ici est d'utiliser un site web et de le parcourir comme un utilisateur a l'aide d'un script.

Selenium est un outil qui permet d'executer des actions script√©es comme le parcours de pages sur des interfaces web.

C'est un outil qui est tr√®s utilis√© dans diff√©rents languages : java, python, javascript,.. 

√ßa s'installe avec pip : `pip3 install selenium`



### Tests d'int√©gration avec selenium

On peut l'utiliser aussi bien pour la r√©alisation de tests fonctionnels : `je me connecte a tel doctolib, je m'attends a trouver 10 profils diff√©rents de m√©decins dans l'affichage`

Regardons ensemble le **getting started** de la doc officielle : <a href="https://selenium-python.readthedocs.io/getting-started.html"> https://selenium-python.readthedocs.io/getting-started.html
</a>

<div class="alert alert-info">
  <strong> Pour aller plus loin </strong> <br/>
    Un autre outil tr√®s complet pour des gros projets python : <a href="https://scrapy.org/">scrapy </a>
</div>

## Protection contre le webscrapping : `Humaniser notre processus`.

La r√©cup√©ration de donn√©es en utilisant du webscrapping peut s'appr√™ter a une attaque informatique de type `DDOS` (Denial Of Service).

Ainsi certains sites ont mis en oeuvre des solutions pour se prot√©ger contre le DDOS et le webscraping **abusif**. 
Ces solutions reposent sur les principes suivants : 

- Les sites bloquent des requ√™tes r√©p√©t√©es sur des intervalles de temps trop proches venant d'une m√™me IP (~m√™me machine)
- Les sites modifient contenu des balises html pour emp√™cher l'automatisation
- Cr√©ation de Honeypot : liens invisibles que seul un bot "cliquerait" pour bloquer les bots/botteurs.
- Authentification exig√©e au bout d'un certain nombre d'usages.

Ils proposent donc diff√©rentes guidelines g√©n√©rales pour les utilisateurs qui souhaitent webscraper : 
- Les sites pr√©cisent ce qu'ils permettent aux robots sur un endpoint particulier le endpoint `robots.txt`, exemple : [https://www.google.com/robots.txt](https://www.google.com/robots.txt)
- Il faut en g√©n√©ral essayer d'espacer un minimum les requ√™tes, il y a en g√©n√©ral des couches r√©seau "Anti DDOS" qui bloquent les requ√™tes venant d'une m√™me IP dans des intervalles de temps ress√©rr√©s.
- Eviter d

## Manipulation des donn√©es




### Regex, Expressions r√©guli√®res : Isoler et capturer dans des chaines caract√®res

<img src="/img/webservice/regex.jpg">
Les expressions r√©guli√®res, ou regex pour faire court, sont des motifs que vous pouvez utiliser pour rechercher du texte dans une cha√Æne de caract√®res. 
On parle assez souvent de `pattern` `matching`. On va donc ici √©laborer des patterns pour r√©cup√©rer les ensembles coh√©rents de chaine de caract√®res qui respectent ce pattern.

Quel int√™ret ? Ici l'on va parcourir des balises html diverses `div` `ul` et l'on va vouloir par exemple r√©cup√©rer les m√©tadonn√©es contenues dans ces balises.

Python prend en charge les expressions r√©guli√®res gr√¢ce au module `re` d√©j√† pr√©sent dans sa biblioth√®que standard.

> C'est un concept qui est pr√©sent dans la plupart des languages. Il est donc r√©utilisable lors de probl√©matiques de traitement de donn√©es au format `str`

#### Syntaxe 

| Ancres | Description                                               |
|--------|-----------------------------------------------------------|
| ^      | D√©but de ligne. Correspond au d√©but d'une cha√Æne de caract√®res. |
| $      | Fin de ligne. Correspond √† la fin d'une cha√Æne de caract√®res. |
| \b     | Limite de mot. Correspond √† la position entre un caract√®re de mot (\w) et un caract√®re qui n'est pas un caract√®re de mot. |

| Symbole sp√©cial | Description                                               |
|-----------------|-----------------------------------------------------------|
| .               | Correspond √† n'importe quel caract√®re, sauf un saut de ligne. |
| *               | Correspond √† z√©ro ou plusieurs occurrences du caract√®re pr√©c√©dent. |
| \d              | Correspond √† un chiffre. √âquivalent √† [0-9].              |
| \D              | Correspond √† tout caract√®re qui n'est pas un chiffre. √âquivalent √† [^0-9]. |
| \w              | Correspond √† un caract√®re alphanum√©rique (lettres, chiffres, soulign√©). √âquivalent √† [a-zA-Z0-9_]. |
| \W              | Correspond √† tout caract√®re qui n'est pas alphanum√©rique. √âquivalent √† [^a-zA-Z0-9_]. |
| \s              | Correspond √† un caract√®re d'espacement (espace, tabulation, retour √† la ligne). |
| \S              | Correspond √† tout caract√®re qui n'est pas un caract√®re d'espacement. |

Exemples d'utilisation : 

- **^abc** : Correspond √† la cha√Æne "abc" au d√©but de la ligne.
- **xyz$** : Correspond √† la cha√Æne "xyz" √† la fin de la ligne.
- **\d{3}** : Correspond √† trois chiffres cons√©cutifs.
- **\w+** : Correspond √† un ou plusieurs caract√®res alphanum√©riques.
- **^toto.*** r√©cup√®re toute la ligne si elle contient toto au d√©but
#### Groupes de Capture :
Les groupes de capture sont utilis√©s pour capturer une partie sp√©cifique d'une correspondance d'expression r√©guli√®re. Ils sont d√©limit√©s par des parenth√®ses.

| Expression r√©guli√®re | Description                                |
|----------------------|--------------------------------------------|
| (.*)                 | Capture toute la chaine                    |
| `<li>(.*?)</li>`     | Capture tout ce qui est entre la balise li |

On peut ensuite utiliser les groupes de capture avec `\1` ou `\0`

Exemple avec le support du cours
```python
import requests
url = "https://conception-logicielle.abrunetti.fr/cours-2024/"
r = requests.get(url)
html = r.text
print(html)
import re
pattern_regex = "<li><a href=\/cours-2024/(.*?)\>"
matches = re.findall(pattern_regex,html)
print(matches)
# [' title=Cours', 'introduction', 'introduction', 'git/', 'portabilite/', 'qualite-automatisation/']
```

<details><summary class="reponse" ><b>Ce qu'il faut d√©sormais √©viter </b></summary>
<p>

### Fonctions natives
La r√©cup√©ration des donn√©es issues d'un site au format `html` est possible par diff√©rents outils de type client `HTTP`. Ces donn√©es ne sont pas exploitables telles quelles, elle n√©cessitent a minima un retraitement par rapport a tous les √©l√©ments d'affichage inutiles pour l'exploitation des donn√©es.

Ce retraitement peut se fait de mani√®re manuelle dans les str, avec les fonctions `split` ou `find`

exemple pour r√©cup√©rer le nombre de parties du cours envoy√©es sur le site : 
```python
import requests
URL_COURS = https://conception-logicielle.abrunetti.fr
reponse_requete_http = requests.get(f"{URL_COURS/docs/presentation-du-cours/}")
r = requests.get(url)
html = r.text
get_h2_parties_du_cours_beginning_index = html.find("<h2 id=\"parties-du-cours\"")
html_apres_parties = html[get_h2_parties_du_cours_beginning_index::]
get_liste_parties_du_cours_start_index = html_apres_parties.find("<ul>") + 4
get_liste_parties_du_cours_end_index = html_apres_parties.find("</ul>")
html_parties_du_cours = html_apres_parties[get_liste_parties_du_cours_start_index:get_liste_parties_du_cours_end_index]
print("nombre de parties du cours : "+str(len(html_parties_du_cours.split("<li>")) - 1))
```

### Manipulation de donn√©es avec un parser : Parser Html avec beautiful soup

<img src="/img/webservice/beautifulsoup.jpg">

Beautiful Soup permet d'encapsuler l'arborescence des √©l√©ments html dans un objet afin de pouvoir assez facilement le parcourir.

C'est une librairie externe on doit l'installer avec `pip` : `pip3 install beautifulsoup4`
```python
from bs4 import BeautifulSoup
import requests

url = "https://conception-logicielle.abrunetti.fr/cours-2024/"
res = requests.get(url)
html = res.text
soup = BeautifulSoup(html, "html.parser")
```

L'objectif de ce parser est de pouvoir r√©aliser du parsing de html et donc de s'abstraire de toutes les r√®gles de style interne a la page html elle m√™me. En effet, certains pourraient vouloir sauter des lignes pour s√©parer diff√©rentes sections alors que d'autres non. Cette information pollue la r√©cup√©ration de donn√©es et donc il est de bon ton de la contr√¥ler.

Ainsi une page comme celle ci : 

devient apr√®s parsing : 

Beautiful soup permet √©galement le requ√™tage des donn√©es r√©cup√©r√©es: 
```python
results = soup.find(id="ResultsContainer")
job_elements = results.find_all("div", class_="card-content")
title_element = job_element.find("h2", class_="title")
print(title_element.text)
```


<div class="alert alert-info">
  <strong> Pour aller plus loin </strong> <br/>
    Doc officielle : <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a>
</div>

## Web Crawler : Automatisation avec Selenium



